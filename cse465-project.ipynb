{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# collection ekta library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding='utf8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \n",
    "#     Preprocess Text Data\n",
    "    \n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    text = text[81:]\n",
    "\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    \n",
    "#   Load the Preprocessed Training data and return them in batches of <batch_size> or less    \n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load in data\n",
    "data_dir = '../input/seinfeld-scripts/Seinfeld_Scripts.txt'\n",
    "text = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "# works with python array\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class _TestNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(_TestNN, self).__init__()\n",
    "        self.decoder = torch.nn.Linear(input_size, output_size)\n",
    "        self.forward_called = False\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        self.forward_called = True\n",
    "        output = self.decoder(nn_input)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "def _print_success_message():\n",
    "    print('Tests Passed')\n",
    "\n",
    "\n",
    "class AssertTest(object):\n",
    "    def __init__(self, params):\n",
    "        self.assert_param_message = '\\n'.join([str(k) + ': ' + str(v) + '' for k, v in params.items()])\n",
    "    \n",
    "    def test(self, assert_condition, assert_message):\n",
    "        assert assert_condition, assert_message + '\\n\\nUnit Test Function Parameters\\n' + self.assert_param_message\n",
    "\n",
    "\n",
    "def test_create_lookup_tables(create_lookup_tables):\n",
    "    test_text = '''\n",
    "        Moe_Szyslak Moe's Tavern Where the elite meet to drink\n",
    "        Bart_Simpson Eh yeah hello is Mike there Last name Rotch\n",
    "        Moe_Szyslak Hold on I'll check Mike Rotch Mike Rotch Hey has anybody seen Mike Rotch lately\n",
    "        Moe_Szyslak Listen you little puke One of these days I'm gonna catch you and I'm gonna carve my name on your back with an ice pick\n",
    "        Moe_Szyslak Whats the matter Homer You're not your normal effervescent self\n",
    "        Homer_Simpson I got my problems Moe Give me another one\n",
    "        Moe_Szyslak Homer hey you should not drink to forget your problems\n",
    "        Barney_Gumble Yeah you should only drink to enhance your social skills'''\n",
    "    \n",
    "    test_text = test_text.lower()\n",
    "    test_text = test_text.split()\n",
    "    \n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(test_text)\n",
    "    \n",
    "    # Check types\n",
    "    assert isinstance(vocab_to_int, dict),\\\n",
    "        'vocab_to_int is not a dictionary.'\n",
    "    assert isinstance(int_to_vocab, dict),\\\n",
    "        'int_to_vocab is not a dictionary.'\n",
    "    \n",
    "    # Compare lengths of dicts\n",
    "    assert len(vocab_to_int) == len(int_to_vocab),\\\n",
    "        'Length of vocab_to_int and int_to_vocab don\\'t match. ' \\\n",
    "        'vocab_to_int is length {}. int_to_vocab is length {}'.format(len(vocab_to_int), len(int_to_vocab))\n",
    "\n",
    "    # Make sure the dicts have the same words\n",
    "    vocab_to_int_word_set = set(vocab_to_int.keys())\n",
    "    int_to_vocab_word_set = set(int_to_vocab.values())\n",
    "\n",
    "    assert not (vocab_to_int_word_set - int_to_vocab_word_set),\\\n",
    "    'vocab_to_int and int_to_vocab don\\'t have the same words.' \\\n",
    "        '{} found in vocab_to_int, but not in int_to_vocab'.format(vocab_to_int_word_set - int_to_vocab_word_set)\n",
    "    assert not (int_to_vocab_word_set - vocab_to_int_word_set),\\\n",
    "        'vocab_to_int and int_to_vocab don\\'t have the same words.' \\\n",
    "        '{} found in int_to_vocab, but not in vocab_to_int'.format(int_to_vocab_word_set - vocab_to_int_word_set)\n",
    "    \n",
    "    # Make sure the dicts have the same word ids\n",
    "    vocab_to_int_word_id_set = set(vocab_to_int.values())\n",
    "    int_to_vocab_word_id_set = set(int_to_vocab.keys())\n",
    "    \n",
    "    assert not (vocab_to_int_word_id_set - int_to_vocab_word_id_set),\\\n",
    "        'vocab_to_int and int_to_vocab don\\'t contain the same word ids.' \\\n",
    "        '{} found in vocab_to_int, but not in int_to_vocab'.format(vocab_to_int_word_id_set - int_to_vocab_word_id_set)\n",
    "    assert not (int_to_vocab_word_id_set - vocab_to_int_word_id_set),\\\n",
    "        'vocab_to_int and int_to_vocab don\\'t contain the same word ids.' \\\n",
    "        '{} found in int_to_vocab, but not in vocab_to_int'.format(int_to_vocab_word_id_set - vocab_to_int_word_id_set)\n",
    "    \n",
    "    # Make sure the dicts make the same lookup\n",
    "    missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
    "    \n",
    "    assert not missmatches,\\\n",
    "        'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(len(missmatches),\n",
    "                                                                                                          *missmatches[0])\n",
    "    \n",
    "    assert len(vocab_to_int) > len(set(test_text))/2,\\\n",
    "        'The length of vocab seems too small.  Found a length of {}'.format(len(vocab_to_int))\n",
    "    \n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_tokenize(token_lookup):\n",
    "    symbols = set(['.', ',', '\"', ';', '!', '?', '(', ')', '-', '\\n'])\n",
    "    token_dict = token_lookup()\n",
    "    \n",
    "    # Check type\n",
    "    assert isinstance(token_dict, dict), \\\n",
    "        'Returned type is {}.'.format(type(token_dict))\n",
    "\n",
    "    # Check symbols\n",
    "    missing_symbols = symbols - set(token_dict.keys())\n",
    "    unknown_symbols = set(token_dict.keys()) - symbols\n",
    "\n",
    "    assert not missing_symbols, \\\n",
    "    'Missing symbols: {}'.format(missing_symbols)\n",
    "    assert not unknown_symbols, \\\n",
    "        'Unknown symbols: {}'.format(unknown_symbols)\n",
    "\n",
    "    # Check values type\n",
    "    bad_value_type = [type(val) for val in token_dict.values() if not isinstance(val, str)]\n",
    "    \n",
    "    assert not bad_value_type,\\\n",
    "        'Found token as {} type.'.format(bad_value_type[0])\n",
    "\n",
    "    # Check for spaces\n",
    "    key_has_spaces = [k for k in token_dict.keys() if ' ' in k]\n",
    "    val_has_spaces = [val for val in token_dict.values() if ' ' in val]\n",
    "    \n",
    "    assert not key_has_spaces,\\\n",
    "        'The key \"{}\" includes spaces. Remove spaces from keys and values'.format(key_has_spaces[0])\n",
    "    assert not val_has_spaces,\\\n",
    "    'The value \"{}\" includes spaces. Remove spaces from keys and values'.format(val_has_spaces[0])\n",
    "    \n",
    "    # Check for symbols in values\n",
    "    symbol_val = ()\n",
    "    for symbol in symbols:\n",
    "        for val in token_dict.values():\n",
    "            if symbol in val:\n",
    "                symbol_val = (symbol, val)\n",
    "\n",
    "    assert not symbol_val,\\\n",
    "    'Don\\'t use a symbol that will be replaced in your tokens. Found the symbol {} in value {}'.format(*symbol_val)\n",
    "    \n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_rnn(RNN, train_on_gpu):\n",
    "    batch_size = 50\n",
    "    sequence_length = 3\n",
    "    vocab_size = 20\n",
    "    output_size=20\n",
    "    embedding_dim=15\n",
    "    hidden_dim = 10\n",
    "    n_layers = 2\n",
    "    \n",
    "    # create test RNN\n",
    "    # params: (vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "    rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "    \n",
    "    # create test input\n",
    "    a = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "    #b = torch.LongTensor(a)\n",
    "    b = torch.from_numpy(a)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    \n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "        b = b.cuda()\n",
    "    \n",
    "    output, hidden_out = rnn(b, hidden)\n",
    "    \n",
    "    assert_test = AssertTest({\n",
    "                             'Input Size': vocab_size,\n",
    "                             'Output Size': output_size,\n",
    "                             'Hidden Dim': hidden_dim,\n",
    "                             'N Layers': n_layers,\n",
    "                             'Batch Size': batch_size,\n",
    "                             'Sequence Length': sequence_length,\n",
    "                             'Input': b})\n",
    "    \n",
    "    # initialization\n",
    "    correct_hidden_size = (n_layers, batch_size, hidden_dim)\n",
    "    assert_condition = hidden[0].size() == correct_hidden_size\n",
    "    assert_message = 'Wrong hidden state size. Expected type {}. Got type {}'.format(correct_hidden_size, hidden[0].size())\n",
    "    assert_test.test(assert_condition, assert_message)\n",
    "    \n",
    "    # output of rnn\n",
    "    correct_hidden_size = (n_layers, batch_size, hidden_dim)\n",
    "    assert_condition = hidden_out[0].size() == correct_hidden_size\n",
    "    assert_message = 'Wrong hidden state size. Expected type {}. Got type {}'.format(correct_hidden_size, hidden_out[0].size())\n",
    "    assert_test.test(assert_condition, assert_message)\n",
    "    \n",
    "    correct_output_size = (batch_size, output_size)\n",
    "    assert_condition = output.size() == correct_output_size\n",
    "    assert_message = 'Wrong output size. Expected type {}. Got type {}'.format(correct_output_size, output.size())\n",
    "    assert_test.test(assert_condition, assert_message)\n",
    "    \n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_forward_back_prop(RNN, forward_back_prop, train_on_gpu):\n",
    "    batch_size = 200\n",
    "    input_size = 20\n",
    "    output_size = 10\n",
    "    sequence_length = 3\n",
    "    embedding_dim=15\n",
    "    hidden_dim = 10\n",
    "    n_layers = 2\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # create test RNN\n",
    "    rnn = RNN(input_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "    \n",
    "    mock_decoder = MagicMock(wraps=_TestNN(input_size, output_size))\n",
    "    if train_on_gpu:\n",
    "        mock_decoder.cuda()\n",
    "    \n",
    "    mock_decoder_optimizer = MagicMock(wraps=torch.optim.Adam(mock_decoder.parameters(), lr=learning_rate))\n",
    "    mock_criterion = MagicMock(wraps=torch.nn.CrossEntropyLoss())\n",
    "    \n",
    "    with patch.object(torch.autograd, 'backward', wraps=torch.autograd.backward) as mock_autograd_backward:\n",
    "        inp = torch.FloatTensor(np.random.rand(batch_size, input_size))\n",
    "        target = torch.LongTensor(np.random.randint(output_size, size=batch_size))\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        loss, hidden_out = forward_back_prop(mock_decoder, mock_decoder_optimizer, mock_criterion, inp, target, hidden)\n",
    "    \n",
    "    assert (hidden_out[0][0]==hidden[0][0]).sum()==batch_size*hidden_dim, 'Returned hidden state is the incorrect size.'\n",
    "    \n",
    "    assert mock_decoder.zero_grad.called or mock_decoder_optimizer.zero_grad.called, 'Didn\\'t set the gradients to 0.'\n",
    "    assert mock_decoder.forward_called, 'Forward propagation not called.'\n",
    "    assert mock_autograd_backward.called, 'Backward propagation not called'\n",
    "    assert mock_decoder_optimizer.step.called, 'Optimization step not performed'\n",
    "    assert type(loss) == float, 'Wrong return type. Expected {}, got {}'.format(float, type(loss))\n",
    "    \n",
    "    _print_success_message()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Table for Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \n",
    "#      lookup tables for vocabulary\n",
    "#     :param text: The text of tv scripts split into words\n",
    "#     :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation For Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    
    "    tokens = dict()\n",
    "    tokens['.'] = '<PERIOD>'\n",
    "    tokens[','] = '<COMMA>'\n",
    "    tokens['\"'] = '<QUOTATION_MARK>'\n",
    "    tokens[';'] = '<SEMICOLON>'\n",
    "    tokens['!'] = '<EXCLAMATION_MARK>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['('] = '<LEFT_PAREN>'\n",
    "    tokens[')'] = '<RIGHT_PAREN>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['-'] = '<DASH>'\n",
    "    tokens['\\n'] = '<NEW_LINE>'\n",
    "    return tokens \n",
   
    "test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process training data\n",
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(train_on_gpu)\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Must train the neural network in GPU otherwise cannot train the network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    " \n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    n_batches = len(words)//batch_size\n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "#         print(\"feature: \",x_batch)\n",
    "        batch_y =  words[idx_end]\n",
    "#         print(\"target: \", batch_y)    \n",
    "        y.append(batch_y)    \n",
    "\n",
    "    # Tensor datasets\n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    # SHUFFLE the training data\n",
    "    data_loader = DataLoader(data, shuffle=False, batch_size=batch_size)\n",
    "    # return a dataloader\n",
    "    return data_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5,lr=0.001):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # define embedding layer        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        # get last batch\n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = rnn(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    # 'clip_grad_norm' helps prevent the exploding gradient problem in LSTMs\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item(), h\n",
    "\n",
    "test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length =  10 # of words in a sequence\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 200\n",
    "hidden_dim = 250\n",
    "n_layers = 2\n",
    "\n",
    "show_every_n_batches = 2000\n",
    "\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "save_model('./trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = load_model('./trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "\n",
    "    rnn.eval()\n",
    "    \n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq).cpu()\n",
    "        \n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu()\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p,top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        current_seq = np.roll(current_seq.cpu().numpy(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a New Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 500\n",
    "prime_word = 'kramer'\n",
    "\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_2.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'./trained_rnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(r'./preprocess.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(r'./generated_script_2.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
